/bin /boot /chef_reboot_override /chef_reboot_required /chef_reboot_trigger /data /dev /etc /fblearner_canary_tmp /fblearner_tmp /gfsai-bistro-east /gfsai-bistro-oregon /gfsai-east /gfsai-flash-east /gfsai-oregon /home /hphp /lib /lib64 /lost+found /mcrouter-web /media /mnt /opt /proc /root /run /sbin /sftp /srv /sys /tmp /usr /var /version-web AutoTune.cpp AutoTune.h AuxIndexStructures.cpp AuxIndexStructures.h benchs BinaryCode.cpp BinaryCode.h c Clustering.cpp Clustering.h example_makefiles FaissAssert.h faiss.h faiss.py gpu hamming.cpp hamming.h Heap.cpp Heap.h Index.cpp IndexFlat.cpp IndexFlat.h Index.h index_io.cpp index_io.h IndexIVF.cpp IndexIVF.h IndexIVFPQ.cpp IndexIVFPQ.h IndexLSH.cpp IndexLSH.h IndexNested.cpp IndexNested.h IndexPQ.cpp IndexPQ.h INSTALL knngraph_nndescent.cpp knngraph_nndescent.h lua Makefile #MetaIndexes.cpp# MetaIndexes.cpp #MetaIndexes.h# MetaIndexes.h PolysemousTraining.cpp PolysemousTraining.h #ProductQuantizer.cpp# ProductQuantizer.cpp ProductQuantizer.h python python_history README scripts shipit-hack.bash shipit-hack.bash~ swigfaiss.swig TARGETS tests tutorial utils.cpp utils.h VectorTransform.cpp VectorTransform.h Copyright (c) 2015-present, Facebook, Inc. AutoTune.cpp AutoTune.h AuxIndexStructures.cpp AuxIndexStructures.h benchs BinaryCode.cpp BinaryCode.h c Clustering.cpp Clustering.h example_makefiles FaissAssert.h faiss.h faiss.py gpu hamming.cpp hamming.h Heap.cpp Heap.h Index.cpp IndexFlat.cpp IndexFlat.h Index.h index_io.cpp index_io.h IndexIVF.cpp IndexIVF.h IndexIVFPQ.cpp IndexIVFPQ.h IndexLSH.cpp IndexLSH.h IndexNested.cpp IndexNested.h IndexPQ.cpp IndexPQ.h INSTALL knngraph_nndescent.cpp knngraph_nndescent.h lua Makefile #MetaIndexes.cpp# MetaIndexes.cpp #MetaIndexes.h# MetaIndexes.h PolysemousTraining.cpp PolysemousTraining.h #ProductQuantizer.cpp# ProductQuantizer.cpp ProductQuantizer.h python python_history README scripts shipit-hack.bash shipit-hack.bash~ swigfaiss.swig TARGETS tests tutorial utils.cpp utils.h VectorTransform.cpp VectorTransform.h All rights reserved. AutoTune.cpp AutoTune.h AuxIndexStructures.cpp AuxIndexStructures.h benchs BinaryCode.cpp BinaryCode.h c Clustering.cpp Clustering.h example_makefiles FaissAssert.h faiss.h faiss.py gpu hamming.cpp hamming.h Heap.cpp Heap.h Index.cpp IndexFlat.cpp IndexFlat.h Index.h index_io.cpp index_io.h IndexIVF.cpp IndexIVF.h IndexIVFPQ.cpp IndexIVFPQ.h IndexLSH.cpp IndexLSH.h IndexNested.cpp IndexNested.h IndexPQ.cpp IndexPQ.h INSTALL knngraph_nndescent.cpp knngraph_nndescent.h lua Makefile #MetaIndexes.cpp# MetaIndexes.cpp #MetaIndexes.h# MetaIndexes.h PolysemousTraining.cpp PolysemousTraining.h #ProductQuantizer.cpp# ProductQuantizer.cpp ProductQuantizer.h python python_history README scripts shipit-hack.bash shipit-hack.bash~ swigfaiss.swig TARGETS tests tutorial utils.cpp utils.h VectorTransform.cpp VectorTransform.h AutoTune.cpp AutoTune.h AuxIndexStructures.cpp AuxIndexStructures.h benchs BinaryCode.cpp BinaryCode.h c Clustering.cpp Clustering.h example_makefiles FaissAssert.h faiss.h faiss.py gpu hamming.cpp hamming.h Heap.cpp Heap.h Index.cpp IndexFlat.cpp IndexFlat.h Index.h index_io.cpp index_io.h IndexIVF.cpp IndexIVF.h IndexIVFPQ.cpp IndexIVFPQ.h IndexLSH.cpp IndexLSH.h IndexNested.cpp IndexNested.h IndexPQ.cpp IndexPQ.h INSTALL knngraph_nndescent.cpp knngraph_nndescent.h lua Makefile #MetaIndexes.cpp# MetaIndexes.cpp #MetaIndexes.h# MetaIndexes.h PolysemousTraining.cpp PolysemousTraining.h #ProductQuantizer.cpp# ProductQuantizer.cpp ProductQuantizer.h python python_history README scripts shipit-hack.bash shipit-hack.bash~ swigfaiss.swig TARGETS tests tutorial utils.cpp utils.h VectorTransform.cpp VectorTransform.h This source code is licensed under the CC-by-NC license found in the AutoTune.cpp AutoTune.h AuxIndexStructures.cpp AuxIndexStructures.h benchs BinaryCode.cpp BinaryCode.h c Clustering.cpp Clustering.h example_makefiles FaissAssert.h faiss.h faiss.py gpu hamming.cpp hamming.h Heap.cpp Heap.h Index.cpp IndexFlat.cpp IndexFlat.h Index.h index_io.cpp index_io.h IndexIVF.cpp IndexIVF.h IndexIVFPQ.cpp IndexIVFPQ.h IndexLSH.cpp IndexLSH.h IndexNested.cpp IndexNested.h IndexPQ.cpp IndexPQ.h INSTALL knngraph_nndescent.cpp knngraph_nndescent.h lua Makefile #MetaIndexes.cpp# MetaIndexes.cpp #MetaIndexes.h# MetaIndexes.h PolysemousTraining.cpp PolysemousTraining.h #ProductQuantizer.cpp# ProductQuantizer.cpp ProductQuantizer.h python python_history README scripts shipit-hack.bash shipit-hack.bash~ swigfaiss.swig TARGETS tests tutorial utils.cpp utils.h VectorTransform.cpp VectorTransform.h LICENSE file in the root directory of this source tree. An additional grant AutoTune.cpp AutoTune.h AuxIndexStructures.cpp AuxIndexStructures.h benchs BinaryCode.cpp BinaryCode.h c Clustering.cpp Clustering.h example_makefiles FaissAssert.h faiss.h faiss.py gpu hamming.cpp hamming.h Heap.cpp Heap.h Index.cpp IndexFlat.cpp IndexFlat.h Index.h index_io.cpp index_io.h IndexIVF.cpp IndexIVF.h IndexIVFPQ.cpp IndexIVFPQ.h IndexLSH.cpp IndexLSH.h IndexNested.cpp IndexNested.h IndexPQ.cpp IndexPQ.h INSTALL knngraph_nndescent.cpp knngraph_nndescent.h lua Makefile #MetaIndexes.cpp# MetaIndexes.cpp #MetaIndexes.h# MetaIndexes.h PolysemousTraining.cpp PolysemousTraining.h #ProductQuantizer.cpp# ProductQuantizer.cpp ProductQuantizer.h python python_history README scripts shipit-hack.bash shipit-hack.bash~ swigfaiss.swig TARGETS tests tutorial utils.cpp utils.h VectorTransform.cpp VectorTransform.h of patent rights can be found in the PATENTS file in the same directory. benchs/ c/ example_makefiles/ gpu/ lua/ python/ scripts/ tests/ tutorial/
// Copyright 2004-present Facebook. All Rights Reserved.

#pragma once

#include "../utils/DeviceTensor.cuh"
#include "../utils/DeviceVector.cuh"
#include "../utils/Float16.cuh"

namespace faiss { namespace gpu {

class GpuResources;

/// Holder of GPU resources for a particular flat index
class FlatIndex {
 public:
  FlatIndex(GpuResources* res,
            int dim,
            bool l2Distance,
            bool useFloat16);

  bool getUseFloat16() const;

  /// Returns the number of vectors we contain
  int getSize() const;

  int getDim() const;

  /// Returns a reference to our vectors currently in use
  Tensor<float, 2, true>& getVectorsFloat32Ref();

#ifdef FAISS_USE_FLOAT16
  /// Returns a reference to our vectors currently in use (useFloat16 mode)
  Tensor<half, 2, true>& getVectorsFloat16Ref();
#endif

  /// Performs a copy of the vectors on the given device, converting
  /// as needed from float16
  DeviceTensor<float, 2, true> getVectorsFloat32Copy(cudaStream_t stream);

  /// Returns only a subset of the vectors
  DeviceTensor<float, 2, true> getVectorsFloat32Copy(int from,
                                                     int num,
                                                     cudaStream_t stream);

  void query(Tensor<float, 2, true>& vecs,
             int k,
             Tensor<float, 2, true>& outDistances,
             Tensor<int, 2, true>& outIndices,
             bool exactDistance,
             int tileSize = -1);

#ifdef FAISS_USE_FLOAT16
  void query(Tensor<half, 2, true>& vecs,
             int k,
             Tensor<half, 2, true>& outDistances,
             Tensor<int, 2, true>& outIndices,
             bool exactDistance,
             int tileSize = -1);
#endif

  /// Add vectors to ourselves; the pointer passed can be on the host
  /// or the device
  void add(const float* data, int numVecs, cudaStream_t stream);

  /// Free all storage
  void reset();

 private:
  /// Collection of GPU resources that we use
  GpuResources* resources_;

  /// Dimensionality of our vectors
  const int dim_;

  /// Float16 data format
  const bool useFloat16_;

  /// L2 or inner product distance?
  bool l2Distance_;

  /// How many vectors we have
  int num_;

  /// The underlying expandable storage
  DeviceVector<char> rawData_;

  /// Vectors currently in rawData_
  DeviceTensor<float, 2, true> vectors_;

#ifdef FAISS_USE_FLOAT16
  /// Vectors currently in rawData_, float16 form
  DeviceTensor<half, 2, true> vectorsHalf_;
#endif

  /// Precomputed L2 norms
  DeviceTensor<float, 1, true> norms_;

#ifdef FAISS_USE_FLOAT16
  /// Precomputed L2 norms, float16 form
  DeviceTensor<half, 1, true> normsHalf_;
#endif
};

} } // namespace
